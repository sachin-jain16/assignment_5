# -*- coding: utf-8 -*-
"""assignment_fashion_mnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MnI5CByY09H03Ub1V1kpVKBo5h67QItX
"""



"""**Dataset**

Fashion MNIST Dataset

**Steps**
1)Data Augmentation
2)Building CNN Model
3) Adding layer of dropout(0.2) and batch normalization
4) Adding layer of dropout(0.3) and batch normalization
"""

from google.colab import files
files.upload()

from keras.datasets import fashion_mnist

(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()

import numpy as np
import cv2

import matplotlib.pyplot as plt
import gzip
import matplotlib.pyplot as plt
from mlxtend.data import loadlocal_mnist





print("X_train-shape:",X_train.shape)
print("X_test-shape:",X_test.shape)
print("Y_train-shape:",Y_train.shape)
print("Y_test-shape:",Y_test.shape)
m_train = X_train.shape[0]
m_test = X_test.shape[0]
print ("Number of training examples: m_train = " + str(m_train))
print ("Number of testing examples: m_test = " + str(m_test))

np.random.seed(0);
indices = list(np.random.randint(m_train,size=9))
print(indices)
for i in range(len(indices)):
    plt.subplot(3,3,i+1)
    plt.imshow(X_train[indices[i]].reshape(28,28),interpolation='none' )
    plt.title("Index {} Class {}".format(indices[i], Y_train[indices[i]]))
    plt.tight_layout()





x_train=X_train.reshape(60000,28,28).copy()
x_test=X_test.reshape(10000,28,28).copy()

print("x_train-shape:",x_train.shape)
print("x_test-shape:",x_test.shape)

# function for fliping the image
def data_augmentation(x_train):
    x_train_aug = x_train
    def flip_image(x_train):
        x_flip =[]   
        for i in range(len(x_train)):
            flipped_img = np.fliplr(x_train[i])
            x_flip.append(flipped_img)
        return x_flip  


# function for horizontal shifting
    def horizontal_shift(x_train):
        x_shift=[]
        WIDTH =28
        HEIGHT =28
        for k in range(len(x_train)):
            for j in range(WIDTH):
                for i in range(HEIGHT):
                    if i < HEIGHT -20:
                        x_train[k][j][i] = x_train[k][j][i+20]
                        x_shift.append(x_train[k])
        return x_shift


# function for adding random noise
    def noise_(x_train):
        m  = len(x_train)
        HEIGHT = 28
        WIDTH  =28
        noise = np.random.randint( 5,size = (m, 28, 28), dtype = 'uint8')
        for i in range(m):
            for j in range(WIDTH):
                for k in range(HEIGHT):
                    if (x_train[i][j][k] <=250):
                        x_train[i][j][k] += noise[i][j][k]
        return x_train


# function for rotating image
    def image_rotation(x_train):
        length,rows,cols = x_train.shape
        rotate_image=[]
        for i in range(length):
            M = cv2.getRotationMatrix2D((cols/2,rows/2),90,1)
            dst = cv2.warpAffine(x_train[i],M,(cols,rows))
            rotate_image.append(dst)
        return rotate_image

# color shifting of the image
    def color_shift(x_train):
        color_contrast=[]
        length,rows,columns =  x_train.shape
        for k in range (length):
            for i in range(28):
                for j in range(28):
                    x_train[k][i][j] = x_train[k][i][j]-8
            color_contrast.append(x_train[k])             
                    
        return color_contrast
    
    #flip_data = flip_image(x_train)
    #x_train_aug = np.vstack((x_train_aug,flip_data))
    #h_shift    = horizontal
    #x_train_aug= np.vstack((x_train_aug,h_shift))
    #noise_data = noise_(x_train)
    #x_train_aug= np.vstack((x_train_aug,noise_data))
    rotate_image = image_rotation(x_train)
    x_train_aug= np.vstack((x_train_aug,rotate_image))
    #color_contrast = color_shift(x_train)
    #x_train_aug= np.vstack((x_train_aug,color_contrast))
    return x_train_aug

x_train_aug = data_augmentation(x_train)
x_train_aug.shape

def y_augmentation(y_train):
    y_list =np.array(Y_train).tolist()
    #print(type(y_list))
    num_of_transformations =int(len(x_train_aug)/len(x_train))
    y_list = y_list*num_of_transformations
    y_train = np.array(y_list)
    y_one_hot = np.eye(10)[y_train]
    y_one_hot
    return y_one_hot

y_train = y_augmentation(Y_train)
print(len(y_train))
y_train.shape

"""# Building CNN model"""

import tensorflow as tf

train_X = x_train_aug.reshape(-1, 28, 28, 1)
train_X.shape

n_classes = 10

x = tf.placeholder("float", [None, 28,28,1])
y = tf.placeholder("float", [None, n_classes])
training_iters = 200 
learning_rate = 0.001 
batch_size = 200

def conv2d(x, W, b, strides=1):
    # Conv2D wrapper, with bias and relu activation
    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')
    x = tf.nn.bias_add(x, b)
    return tf.nn.relu(x)
def maxpool2d(x, k=2):
    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],padding='SAME')

weights = {
    'wc1': tf.get_variable('W0', shape=(3,3,1,32), initializer=tf.contrib.layers.xavier_initializer()), 
    'wc2': tf.get_variable('W1', shape=(3,3,32,64), initializer=tf.contrib.layers.xavier_initializer()), 
    'wc3': tf.get_variable('W2', shape=(3,3,64,128), initializer=tf.contrib.layers.xavier_initializer()), 
    'wd1': tf.get_variable('W3', shape=(4*4*128,128), initializer=tf.contrib.layers.xavier_initializer()), 
    'out': tf.get_variable('W6', shape=(128,n_classes), initializer=tf.contrib.layers.xavier_initializer()), 
}
biases = {
    'bc1': tf.get_variable('B0', shape=(32), initializer=tf.contrib.layers.xavier_initializer()),
    'bc2': tf.get_variable('B1', shape=(64), initializer=tf.contrib.layers.xavier_initializer()),
    'bc3': tf.get_variable('B2', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),
    'bd1': tf.get_variable('B3', shape=(128), initializer=tf.contrib.layers.xavier_initializer()),
    'out': tf.get_variable('B4', shape=(10), initializer=tf.contrib.layers.xavier_initializer()),
}

x_aug = tf.cast(train_X,tf.float32)
def conv_net(x, weights, biases):  

    # here we call the conv2d function we had defined above and pass the input image x, weights wc1 and bias bc1.
    conv1 = conv2d(x, weights['wc1'], biases['bc1'])
    print(conv1.shape)
    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 14*14 matrix.
    conv1 = maxpool2d(conv1, k=2)
    print(conv1.shape)

    # Convolution Layer
    # here we call the conv2d function we had defined above and pass the input image x, weights wc2 and bias bc2.
    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])
    print(conv2.shape)
    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 7*7 matrix.
    conv2 = maxpool2d(conv2, k=2)
    print(conv2.shape)

    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])
    print(conv3.shape)
    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 4*4.
    conv3 = maxpool2d(conv3, k=2)
    print(conv3.shape)


    # Fully connected layer
    # Reshape conv2 output to fit fully connected layer input
    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])
    print(fc1.shape)
    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])
    print(fc1.shape)
    fc1 = tf.nn.relu(fc1)
    print(fc1.shape)
    # Output, class prediction
    # finally we multiply the fully connected layer with the weights and add a bias term. 
    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])
    return out

#pred = conv_net(x_train_aug, weights, biases)
print(x_aug)
weights['wc1'].dtype
biases["bc1"].dtype

print(weights['wc1'])

pred = conv_net(x, weights, biases)

cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))

optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

#Here you check whether the index of the maximum value of the predicted image is equal to the actual labelled image. and both will be a column vector.
correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))

#calculate accuracy across all the given images and average them out. 
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

# Initializing the variables
init = tf.global_variables_initializer()

m = int(len(x_train_aug)/batch_size)
print(m)

x_test = X_test.reshape(-1, 28, 28, 1)
x_test.shape

y_test = np.eye(10)[Y_test]
y_test.shape

iterations =60
with tf.Session() as sess:
    sess.run(init) 
    train_loss = []
    test_loss = []
    train_accuracy = []
    test_accuracy = []
    saver=tf.train.Saver()
    initial =0
    summary_writer = tf.summary.FileWriter('./Output', sess.graph)
    
    for i in range(iterations):
        for batch in range(m):
            batch_x = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X))]
            batch_y = y_train[batch*batch_size:min((batch+1)*batch_size,len(y_train))]    
            # Run optimization op (backprop).
                # Calculate batch loss and accuracy
            opt = sess.run(optimizer, feed_dict={x: batch_x,
                                                              y: batch_y})
            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,
                                                              y: batch_y})
        print("Iter " + str(i) + ", Loss= " + \
                      "{:.6f}".format(loss) + ", Training Accuracy= " + \
                      "{:.5f}".format(acc))
        print("Optimization Finished!")
        
        loss_val,acc_val=sess.run([cost,accuracy],feed_dict={x:x_test,y:y_test})
        if acc_val>initial:
            print("accuracy:",acc_val)
            saver.save(sess,'./result')
        

        # Calculate accuracy for all 10000 mnist test images
        #test_acc,valid_loss = sess.run([accuracy,cost], feed_dict={x: test_X,y : test_y})
        #train_loss.append(loss)
        #test_loss.append(valid_loss)
        #train_accuracy.append(acc)
        #test_accuracy.append(test_acc)
        #print("Testing Accuracy:","{:.5f}".format(test_acc))
    summary_writer.close()

tf.reset_default_graph()
imported_graph=tf.train.import_meta_graph('result.meta')
with tf.Session() as sess:
  imported_graph.restore(sess,'./result')
  we=sess.run('W0/Initializer/random_uniform/shape:0')
  print(we)
  #loss_val,acc_val=sess.run([cost,accuracy],feed_dict={'Placeholder:0':x_test,'Placeholder_1:0':y_test})

def conv_net_modified(x, weights, biases,dropout):  

    # here we call the conv2d function we had defined above and pass the input image x, weights wc1 and bias bc1.
    conv1 = conv2d(x, weights['wc1'], biases['bc1'])
    print(conv1.shape)
    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 14*14 matrix.
    conv1 = maxpool2d(conv1, k=2)
    print(conv1.shape)
    #adding dropout layer
    conv1 = tf.layers.dropout(conv1,dropout)
    # batch normalization
    conv1 = tf.layers.batch_normalization(conv1)

    # Convolution Layer
    # here we call the conv2d function we had defined above and pass the input image x, weights wc2 and bias bc2.
    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])
    print(conv2.shape)
    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 7*7 matrix.
    conv2 = maxpool2d(conv2, k=2)
    print(conv2.shape)
    #adding dropout layer
    conv2 = tf.layers.dropout(conv2,dropout)
    # batch normalization
    conv2 = tf.layers.batch_normalization(conv2)
    

    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])
    print(conv3.shape)
    # Max Pooling (down-sampling), this chooses the max value from a 2*2 matrix window and outputs a 4*4.
    conv3 = maxpool2d(conv3, k=2)
    print(conv3.shape)
    #adding dropout layer
    conv3 = tf.layers.dropout(conv3,dropout)
    # batch normalization
    conv3 = tf.layers.batch_normalization(conv3)


    # Fully connected layer
    # Reshape conv2 output to fit fully connected layer input
    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]])
    print(fc1.shape)
    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])
    print(fc1.shape)
    fc1 = tf.nn.relu(fc1)
    print(fc1.shape)
    # Output, class prediction
    # finally we multiply the fully connected layer with the weights and add a bias term. 
    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])
    return out

pred_m = conv_net_modified(x, weights, biases,0.2)

cost_m = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred_m, labels=y))

optimizer_m = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_m)

#Here you check whether the index of the maximum value of the predicted image is equal to the actual labelled image. and both will be a column vector.
correct_prediction = tf.equal(tf.argmax(pred_m, 1), tf.argmax(y, 1))

#calculate accuracy across all the given images and average them out. 
accuracy_m = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

iterations = 10
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    
    train_loss_modified = []
    train_accuracy_modified = []
    test_loss_modified = []
    test_accuracy_modified = []
    saver_modified = tf.train.Saver()
    accu_modified = 0
    
    for i in range(iterations):
        for batch in range(m):
            x_batch = train_X[batch*batch_size:min((batch+1)*batch_size, len(train_X))]
            y_batch = y_train[batch*batch_size:min((batch+1)*batch_size, len(y_train))]
            
            opt_modified = sess.run(optimizer_m, feed_dict={x:x_batch, y:y_batch})
            
            loss_m, acc_m = sess.run([cost_m, accuracy_m], feed_dict={x:x_batch, y:y_batch})
        
        print("epoch " + str(i+1) + ", Loss= " + "{:.6f}".format(loss_m) + ", Training Accuracy = " + \
             "{:.5f}".format(acc_m))
        print("Optimization Finished!")
        
        test_acc_m, valid_loss_m = sess.run([accuracy_m, cost_m], feed_dict={x:x_test, y:y_test})
        
        train_loss_modified.append(loss_m)
        test_loss_modified.append(valid_loss_m)
        train_accuracy_modified.append(acc_m)
        test_accuracy_modified.append(test_acc_m)
        
        print("Testing Accuracy:", "{:.5f}".format(test_acc_m))
        
        if(test_acc_m > accu_modified):
            accu_modified = test_acc_m
            saver_modified.save(sess,'result_modified')

"""**Drpout =0.3, Batch Normalization**"""

pred_m_1 = conv_net_modified(x, weights, biases,0.3)

cost_m_1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred_m_1, labels=y))

optimizer_m_1 = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_m_1)

#Here you check whether the index of the maximum value of the predicted image is equal to the actual labelled image. and both will be a column vector.
correct_prediction_m_1 = tf.equal(tf.argmax(pred_m, 1), tf.argmax(y, 1))

#calculate accuracy across all the given images and average them out. 
accuracy_m_1 = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

iterations = 10
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    
    train_loss_modified = []
    train_accuracy_modified = []
    test_loss_modified = []
    test_accuracy_modified = []
    saver_modified = tf.train.Saver()
    accu_modified = 0
    
    for i in range(iterations):
        for batch in range(m):
            x_batch = train_X[batch*batch_size:min((batch+1)*batch_size, len(train_X))]
            y_batch = y_train[batch*batch_size:min((batch+1)*batch_size, len(y_train))]
            
            opt_modified = sess.run(optimizer_m_1, feed_dict={x:x_batch, y:y_batch})
            
            loss_m, acc_m = sess.run([cost_m_1, accuracy_m_1], feed_dict={x:x_batch, y:y_batch})
        
        print("epoch " + str(i+1) + ", Loss= " + "{:.6f}".format(loss_m) + ", Training Accuracy = " + \
             "{:.5f}".format(acc_m))
        print("Optimization Finished!")
        
        test_acc_m, valid_loss_m = sess.run([accuracy_m_1, cost_m_1], feed_dict={x:x_test, y:y_test})
        
        train_loss_modified.append(loss_m)
        test_loss_modified.append(valid_loss_m)
        train_accuracy_modified.append(acc_m)
        test_accuracy_modified.append(test_acc_m)
        
        print("Testing Accuracy:", "{:.5f}".format(test_acc_m))
        
        if(test_acc_m > accu_modified):
            accu_modified = test_acc_m
            saver_modified.save(sess,'result_modified_1')

